AWSTemplateFormatVersion: '2010-09-09'
Parameters:
  Environment:
    Type: String
    Default: dev

Resources:
  LambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                Resource: 
                  - !Sub 'arn:aws:s3:::pdf-input-${Environment}-${AWS::AccountId}-${AWS::Region}/*'
                  - !Sub 'arn:aws:s3:::pdf-output-${Environment}-${AWS::AccountId}-${AWS::Region}/*'
        - PolicyName: DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:PutItem
                  - dynamodb:GetItem
                  - dynamodb:UpdateItem
                Resource: !Sub 'arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/pdf-processing-${Environment}'
        - PolicyName: LambdaInvoke
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !Sub 'arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:*-processor-${Environment}'

  # Router Lambda
  DocumentRouter:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'document-router-${Environment}'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          from datetime import datetime
          
          lambda_client = boto3.client('lambda')
          dynamodb = boto3.resource('dynamodb')
          
          def lambda_handler(event, context):
              print(f"Router Event: {json.dumps(event)}")
              
              function_name = context.function_name
              env = function_name.split('-')[-1]
              
              table_name = f'pdf-processing-{env}'
              table = dynamodb.Table(table_name)
              
              for record in event['Records']:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  print(f"Routing file: {key}")
                  
                  # Log initial routing
                  table.put_item(Item={
                      'file_key': key,
                      'status': 'ROUTING',
                      'timestamp': datetime.now().isoformat(),
                      'bucket': bucket
                  })
                  
                  # Determine processor based on file extension
                  file_ext = key.lower().split('.')[-1]
                  
                  processor_map = {
                      'pdf': f'pdf-processor-{env}',
                      'png': f'image-processor-{env}',
                      'jpg': f'image-processor-{env}',
                      'jpeg': f'image-processor-{env}',
                      'csv': f'csv-processor-{env}',
                      'txt': f'text-processor-{env}',
                      'md': f'text-processor-{env}'
                  }
                  
                  processor_function = processor_map.get(file_ext)
                  
                  if processor_function:
                      try:
                          # Invoke specific processor
                          lambda_client.invoke(
                              FunctionName=processor_function,
                              InvocationType='Event',  # Async
                              Payload=json.dumps(event)
                          )
                          
                          table.put_item(Item={
                              'file_key': key,
                              'status': 'ROUTED',
                              'timestamp': datetime.now().isoformat(),
                              'bucket': bucket,
                              'processor': processor_function
                          })
                          
                      except Exception as e:
                          print(f"Routing error for {key}: {str(e)}")
                          table.put_item(Item={
                              'file_key': key,
                              'status': 'ROUTING_FAILED',
                              'timestamp': datetime.now().isoformat(),
                              'bucket': bucket,
                              'error': str(e)
                          })
                  else:
                      print(f"No processor for file type: {file_ext}")
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'UNSUPPORTED_TYPE',
                          'timestamp': datetime.now().isoformat(),
                          'bucket': bucket,
                          'file_type': file_ext
                      })
              
              return {"statusCode": 200, "body": "Routing complete"}

  # PDF Processor with Layer
  PDFProcessor:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'pdf-processor-${Environment}'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Timeout: 300
      MemorySize: 1024
      Layers:
        - !ImportValue 
            Fn::Sub: 'pdf-layers-${Environment}-PDFProcessingLayer'
      Code:
        ZipFile: |
          import json
          import boto3
          import PyPDF2
          import pdfplumber
          import io
          from datetime import datetime
          
          s3 = boto3.client('s3')
          dynamodb = boto3.resource('dynamodb')
          
          def extract_text_pypdf2(pdf_bytes):
              """Extract text using PyPDF2"""
              pdf_file = io.BytesIO(pdf_bytes)
              pdf_reader = PyPDF2.PdfReader(pdf_file)
              text = ""
              for page in pdf_reader.pages:
                  text += page.extract_text() + "\n"
              return text
          
          def extract_text_pdfplumber(pdf_bytes):
              """Extract text using pdfplumber (better for tables)"""
              pdf_file = io.BytesIO(pdf_bytes)
              text = ""
              with pdfplumber.open(pdf_file) as pdf:
                  for page in pdf.pages:
                      page_text = page.extract_text()
                      if page_text:
                          text += page_text + "\n"
              return text
          
          def lambda_handler(event, context):
              print(f"PDF Processor Event: {json.dumps(event)}")
              
              function_name = context.function_name
              env = function_name.split('-')[-1]
              
              table_name = f'pdf-processing-{env}'
              table = dynamodb.Table(table_name)
              
              for record in event['Records']:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  try:
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'PDF_PROCESSING',
                          'timestamp': datetime.now().isoformat(),
                          'processor': 'pdf-processor'
                      })
                      
                      # Download PDF from S3
                      response = s3.get_object(Bucket=bucket, Key=key)
                      pdf_bytes = response['Body'].read()
                      
                      # Extract text using both methods
                      try:
                          text_pypdf2 = extract_text_pypdf2(pdf_bytes)
                          text_pdfplumber = extract_text_pdfplumber(pdf_bytes)
                          
                          # Use pdfplumber result if it's longer (often better)
                          extracted_text = text_pdfplumber if len(text_pdfplumber) > len(text_pypdf2) else text_pypdf2
                          
                          if not extracted_text.strip():
                              extracted_text = "No text could be extracted from this PDF"
                          
                      except Exception as extract_error:
                          extracted_text = f"Text extraction failed: {str(extract_error)}"
                      
                      # Create analysis
                      analysis = f"PDF Analysis for: {key}\n"
                      analysis += f"File size: {len(pdf_bytes)} bytes\n"
                      analysis += f"Text length: {len(extracted_text)} characters\n"
                      analysis += f"Word count: {len(extracted_text.split())}\n"
                      analysis += f"\nExtracted Text:\n{extracted_text}"
                      
                      output_bucket = bucket.replace('input', 'output')
                      output_key = f"pdf-extracted/{key}.txt"
                      
                      s3.put_object(
                          Bucket=output_bucket,
                          Key=output_key,
                          Body=analysis,
                          ContentType='text/plain'
                      )
                      
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'PDF_COMPLETED',
                          'timestamp': datetime.now().isoformat(),
                          'output_key': output_key,
                          'text_length': len(extracted_text),
                          'word_count': len(extracted_text.split()),
                          'processor': 'pdf-processor'
                      })
                      
                  except Exception as e:
                      print(f"PDF processing error: {str(e)}")
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'PDF_FAILED',
                          'timestamp': datetime.now().isoformat(),
                          'error': str(e),
                          'processor': 'pdf-processor'
                      })
              
              return {"statusCode": 200, "body": "PDF processing complete"}

  # Image Processor with Layer
  ImageProcessor:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'image-processor-${Environment}'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Timeout: 300
      MemorySize: 2048
      Layers:
        - !ImportValue 
            Fn::Sub: 'pdf-layers-${Environment}-ImageProcessingLayer'
      Environment:
        Variables:
          TESSDATA_PREFIX: '/opt/python/usr/share/tesseract-ocr/4.00/tessdata'
      Code:
        ZipFile: |
          import json
          import boto3
          import pytesseract
          from PIL import Image
          import io
          from datetime import datetime
          
          s3 = boto3.client('s3')
          dynamodb = boto3.resource('dynamodb')
          
          def lambda_handler(event, context):
              print(f"Image Processor Event: {json.dumps(event)}")
              
              function_name = context.function_name
              env = function_name.split('-')[-1]
              
              table_name = f'pdf-processing-{env}'
              table = dynamodb.Table(table_name)
              
              for record in event['Records']:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  try:
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'IMAGE_PROCESSING',
                          'timestamp': datetime.now().isoformat(),
                          'processor': 'image-processor'
                      })
                      
                      # Download image from S3
                      response = s3.get_object(Bucket=bucket, Key=key)
                      image_bytes = response['Body'].read()
                      
                      # Open image with Pillow
                      image = Image.open(io.BytesIO(image_bytes))
                      
                      # Get image info
                      image_info = {
                          'format': image.format,
                          'size': image.size,
                          'mode': image.mode
                      }
                      
                      # Perform OCR
                      try:
                          extracted_text = pytesseract.image_to_string(image)
                          if not extracted_text.strip():
                              extracted_text = "No text detected in image"
                      except Exception as ocr_error:
                          extracted_text = f"OCR failed: {str(ocr_error)}\nNote: Tesseract may not be properly configured in Lambda layer"
                      
                      # Create analysis
                      analysis = f"Image OCR Analysis for: {key}\n"
                      analysis += f"Format: {image_info['format']}\n"
                      analysis += f"Size: {image_info['size'][0]}x{image_info['size'][1]}\n"
                      analysis += f"Mode: {image_info['mode']}\n"
                      analysis += f"File size: {len(image_bytes)} bytes\n"
                      analysis += f"Text length: {len(extracted_text)} characters\n"
                      analysis += f"Word count: {len(extracted_text.split())}\n"
                      analysis += f"\nExtracted Text:\n{extracted_text}"
                      
                      output_bucket = bucket.replace('input', 'output')
                      output_key = f"image-extracted/{key}.txt"
                      
                      s3.put_object(
                          Bucket=output_bucket,
                          Key=output_key,
                          Body=analysis,
                          ContentType='text/plain'
                      )
                      
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'IMAGE_COMPLETED',
                          'timestamp': datetime.now().isoformat(),
                          'output_key': output_key,
                          'text_length': len(extracted_text),
                          'word_count': len(extracted_text.split()),
                          'image_format': image_info['format'],
                          'image_size': f"{image_info['size'][0]}x{image_info['size'][1]}",
                          'processor': 'image-processor'
                      })
                      
                  except Exception as e:
                      print(f"Image processing error: {str(e)}")
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'IMAGE_FAILED',
                          'timestamp': datetime.now().isoformat(),
                          'error': str(e),
                          'processor': 'image-processor'
                      })
              
              return {"statusCode": 200, "body": "Image processing complete"}

  # CSV Processor
  CSVProcessor:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'csv-processor-${Environment}'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Code:
        ZipFile: |
          import json
          import boto3
          import csv
          import io
          from datetime import datetime
          
          s3 = boto3.client('s3')
          dynamodb = boto3.resource('dynamodb')
          
          def lambda_handler(event, context):
              print(f"CSV Processor Event: {json.dumps(event)}")
              
              function_name = context.function_name
              env = function_name.split('-')[-1]
              
              table_name = f'pdf-processing-{env}'
              table = dynamodb.Table(table_name)
              
              for record in event['Records']:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  try:
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'CSV_PROCESSING',
                          'timestamp': datetime.now().isoformat(),
                          'processor': 'csv-processor'
                      })
                      
                      response = s3.get_object(Bucket=bucket, Key=key)
                      csv_content = response['Body'].read().decode('utf-8')
                      
                      # Parse CSV and create summary
                      csv_reader = csv.reader(io.StringIO(csv_content))
                      rows = list(csv_reader)
                      
                      summary = f"CSV Analysis for: {key}\n"
                      summary += f"Total rows: {len(rows)}\n"
                      if rows:
                          summary += f"Columns: {len(rows[0])}\n"
                          summary += f"Headers: {', '.join(rows[0])}\n"
                          summary += f"\nFirst 5 rows:\n"
                          for i, row in enumerate(rows[:6]):  # Header + 5 rows
                              summary += f"Row {i}: {', '.join(row)}\n"
                      
                      output_bucket = bucket.replace('input', 'output')
                      output_key = f"csv-analyzed/{key}_summary.txt"
                      
                      s3.put_object(
                          Bucket=output_bucket,
                          Key=output_key,
                          Body=summary,
                          ContentType='text/plain'
                      )
                      
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'CSV_COMPLETED',
                          'timestamp': datetime.now().isoformat(),
                          'output_key': output_key,
                          'row_count': len(rows),
                          'processor': 'csv-processor'
                      })
                      
                  except Exception as e:
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'CSV_FAILED',
                          'timestamp': datetime.now().isoformat(),
                          'error': str(e),
                          'processor': 'csv-processor'
                      })
              
              return {"statusCode": 200, "body": "CSV processing complete"}

  # Text Processor
  TextProcessor:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub 'text-processor-${Environment}'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt LambdaRole.Arn
      Code:
        ZipFile: |
          import json
          import boto3
          from datetime import datetime
          
          s3 = boto3.client('s3')
          dynamodb = boto3.resource('dynamodb')
          
          def lambda_handler(event, context):
              print(f"Text Processor Event: {json.dumps(event)}")
              
              function_name = context.function_name
              env = function_name.split('-')[-1]
              
              table_name = f'pdf-processing-{env}'
              table = dynamodb.Table(table_name)
              
              for record in event['Records']:
                  bucket = record['s3']['bucket']['name']
                  key = record['s3']['object']['key']
                  
                  try:
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'TEXT_PROCESSING',
                          'timestamp': datetime.now().isoformat(),
                          'processor': 'text-processor'
                      })
                      
                      response = s3.get_object(Bucket=bucket, Key=key)
                      text_content = response['Body'].read().decode('utf-8')
                      
                      # Basic text analysis
                      lines = text_content.split('\n')
                      words = text_content.split()
                      
                      analysis = f"Text Analysis for: {key}\n"
                      analysis += f"Characters: {len(text_content)}\n"
                      analysis += f"Lines: {len(lines)}\n"
                      analysis += f"Words: {len(words)}\n"
                      analysis += f"\nContent:\n{text_content}"
                      
                      output_bucket = bucket.replace('input', 'output')
                      output_key = f"text-analyzed/{key}_analysis.txt"
                      
                      s3.put_object(
                          Bucket=output_bucket,
                          Key=output_key,
                          Body=analysis,
                          ContentType='text/plain'
                      )
                      
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'TEXT_COMPLETED',
                          'timestamp': datetime.now().isoformat(),
                          'output_key': output_key,
                          'word_count': len(words),
                          'processor': 'text-processor'
                      })
                      
                  except Exception as e:
                      table.put_item(Item={
                          'file_key': key,
                          'status': 'TEXT_FAILED',
                          'timestamp': datetime.now().isoformat(),
                          'error': str(e),
                          'processor': 'text-processor'
                      })
              
              return {"statusCode": 200, "body": "Text processing complete"}

Outputs:
  DocumentRouter:
    Value: !Ref DocumentRouter
    Export:
      Name: !Sub '${AWS::StackName}-DocumentRouter'
  PDFProcessor:
    Value: !Ref PDFProcessor
    Export:
      Name: !Sub '${AWS::StackName}-PDFProcessor'
  ImageProcessor:
    Value: !Ref ImageProcessor
    Export:
      Name: !Sub '${AWS::StackName}-ImageProcessor'
  CSVProcessor:
    Value: !Ref CSVProcessor
    Export:
      Name: !Sub '${AWS::StackName}-CSVProcessor'
  TextProcessor:
    Value: !Ref TextProcessor
    Export:
      Name: !Sub '${AWS::StackName}-TextProcessor'